
\graphicspath{ {6chapterExperimental/image/} }
\chapter{Experimental result}
\section{Data set} \label{test_set}
To evaluate our method, we use the publicly accessible benchmark of natural scenes containing text of the ICDAR 2013 competitions. The data set comes from the second challenge: "Focused Scene Text". The test set contains 233 scene images focused on text on banners, signs or labels in real life, most of them are in the horizontal direction. The ground truth was created manually by observing and painting each character pixels. This process made the ground-truth of ICDAR 2013 data set more reliable than the ICDAR 2011.   
 
\section{Evaluation protocols}
In this section, we summarize the protocol which is used for text detection and recognition evaluation. We use the IDCAR protocol DelEval by Wolf et al \cite{WolfIJDAR2006} which is most commonly adopted. DelEval detection protocol supports both one-to-one and one-to-many matches between the ground truth and detection as in Figure \ref{fig:GroundTruthMatch}. It also considers over-split and over-merge of detection.
\begin{figure}
\begin{center}


	\begin{subfigure}[b]{\textwidth}
	 	\includegraphics[scale=0.5]{oneOne.png} \centering \caption{one-to-one match}\label{fig:oneOne} \end{subfigure}
		
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[scale=0.5]{manyOne.png} \centering	 \caption{a merge: a one-to-many match with one detected rectangle}\label{fig:manyOne} \end{subfigure}		 	
	 	
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[scale=0.5]{oneMany.png} \centering	 \caption{a split: a one-to-many match with one ground truth rectangle.}\label{fig:oneMany} \end{subfigure}	


	
	\begin{subfigure}[b]{\textwidth}
		\includegraphics[height = 1cm]{comment.png} \centering	 \caption{}\label{fig:comment} \end{subfigure}
\end{center}
	\caption[Example of ground truth match types] {Different match types between ground truth rectangles and detected rectangles}
	\label{fig:GroundTruthMatch}
\end{figure}

\section{Results and comments}
\subsection{Outputs of the algorithm}
Through our process, the input image (Figure \ref{fig:inputImage}) will be converted into gray level image ( Figure \ref{fig:grayScale}), then the interpolated Laplacian map will be created (Figure \ref{fig:Laplacian}). Along with the laplacian map, a gradient map \ref{fig:gradient} will also be calculated for addition information. Through experiments, the used structural element is a 11x11 square for both Laplacian and gradient as this size is large enough to ignore small component but does not destroy text structure. Components will be labeled \ref{fig:BoundingBoxOfCharacter}. During the labeling process, some attributes described in section \ref{labeling}, will be checked during the contour following process. Contour of removed components can be seen in \ref{fig:labeling_contour} (red contour). As the Laplacian is sensitive to noise so there are numerous components has been introduced, they are mostly removed by thresholding the value obtain by following the outside contour of each component (bounding boxes of components will also be calculated at the same time\ref{fig:BoundingBoxOfCharacter}). When we process the example image \ref{fig:inputImage} from the ICDAR2013 set, 14650 components are removed because they are too small, 12623 components which pass the size threshold are removed by gradient threshold, 102 other components are removed by the ratio criteria, only 149 components are kept as character candidates. 


\begin{figure}

	\begin{subfigure}[b]{0.45\textwidth}
	 	\includegraphics[width=7cm]{output/img_20.jpg} \caption{Original image}\label{fig:inputImage} \end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	 	\includegraphics[width=7cm]{output/Gau20.png} \caption{Gray level image}\label{fig:grayScale} \end{subfigure}	 	
\centering		
		
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=7cm]{output/lap20.png}  \caption{Morphological Laplacian}\label{fig:Laplacian} \end{subfigure}	
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=7cm]{output/grad20.png}  \caption{Morphological gradient}\label{fig:gradient} \end{subfigure}	
\centering

	\caption[Outputs of the labeling process] {Outputs of the labeling process}
	\label{fig:output}
\end{figure}


Components will grouped into words candidates \ref{fig:BoundingBoxOfWordCandidateOnLabel} using algorithm in section \ref{Grouping}. The bounding box will be scaled back to the original resolution (before interpolation) of input image \ref{fig:BoundingBoxOfWordCandidate}.


\begin{figure}

	\begin{subfigure}[t]{0.45\textwidth}
	\captionsetup{width=0.85\textwidth} 
		\includegraphics[width=7cm]{output/lab20TextBorder.png}  \caption{Components labels with their bounding boxes (shown in red)}\label{fig:BoundingBoxOfCharacter} \end{subfigure}	
	\begin{subfigure}[t]{0.45\textwidth} 
	\captionsetup{width=0.80\textwidth} 
	\includegraphics[width=7cm]{output/lab20Cropped.png}  
	\caption{zoom in at the number 88, with contour of components removed shown. Components which do not pass gradient threshold are shown in red, which do not pass size threshold are shown in black and which do not pass ratio threshold are shown in blue}\label{fig:labeling_contour} \end{subfigure}	
\centering	


	\begin{subfigure}[b]{0.45\textwidth}
	\captionsetup{width=0.85\textwidth} 
		\includegraphics[width=7cm]{output/lab20WordLabeled.png}  \caption{Bounding boxes of word candidate shown in green}\label{fig:BoundingBoxOfWordCandidateOnLabel} \end{subfigure}
	\begin{subfigure}[b]{0.45\textwidth}
	\captionsetup{width=0.85\textwidth}
		\includegraphics[width=7cm]{output/20.png}  \caption{Bounding boxes of word candidate on original image}\label{fig:BoundingBoxOfWordCandidate} \end{subfigure}			
\centering	
		
	\caption[Word candidates output] {Word candidates output}
	\label{fig:Wordcandidatesoutput}
\end{figure}
\subsection{Modification of algorithms}
The original approach removes most of weak components introduced by the laplacian, but the results show that high number of false positives remains. We then try to further eliminate the false positives by 2 approaches. First, we use a classic method: blurring out the original image with a small Gaussian kernel (3 pixels) to reduce noise. Although it destroys the shape of component and modifies contours, the a small kernel keeps this modifications light enough. We test several combinations using the same threshold to find the best one:

\begin{itemize}
\item {\textbf{CP1}} : Laplacian and gradient are calculated from the original gray level image.
\item {\textbf{CP2}} : Laplacian is calculated from the original image and gradient is calculated from the blurred image.
\item {\textbf{CP3}} : Laplacian and gradient are calculated from the blurred image.
\item {\textbf{CP4}} : Laplacian is calculated from the blurred image and gradient is calculated from the original image.
\end{itemize}

Second, we use the Laplacian value of each component as additional information to remove components having low contrast. The laplacian was first normalized into interval [-255;255] . At first, we uses the average of 10\% strongest points of that region as a feature to decide if the zero crossing is strong but then we found out that the peak value of each region is enough. We called the first approach average laplacian and the second one peak laplacian.

\subsection{Experimental}
The bounding boxes are extracted and compared to the ground truth provided by ICDAR \textit{Robust reading competition}, using the DetEval protocol by $Wolf et al$ \cite{WolfIJDAR2006}

\begin {table}[H]
\caption{Performance of our experiments}\label{tab:performanceOur} 
\begin{tabular}{|c|c|c|c|}
\hline 
\textbf{Method} & \textbf{Recall} & \textbf{Precision} & \textbf{F-measure)} \\ 
\hline 
CP3 + Average Laplacian, threshold = 70 & 63.96\% & 36.50\% & \textbf{46.48\%} \\ 
\hline 
CP3 + Average Laplacian, threshold = 65  & 64.15\% & 34.71\% & 45.05\%\\ 
\hline 
CP3 & 65.06\% & 27.75\% & 38.91\% \\ 
\hline
Average Laplacian threshold = 65  & 69.97\% & 15.51\% & 25.39\% \\ 
\hline  
CP2 & 63.78\% & 12.74\% & 21.23\% \\ 
\hline 
CP4 & 69.79\% & 9.22\% & 16.29\% \\ 
\hline 
Propose approach \textbf{(CP1)} &\textbf{ 72.15\%} & 6.11\% & 11.26\% \\ 
\hline 
\end{tabular} 

\end{table}

The original approach (CP1) reaches the recall rate of 71.71\% which shows attractive prospect. But there are too many false positives that decrease the overall performance. It is caused by the high amount of detected false component, especially in images with complex background. It does not only decrease the precision but also make the grouping process much more expensive. For example, 3579 components was labeled for image No.218 in the ICDAR2013 data set (Section \ref{test_set}).


Other modifications arm to increase the precision. Average laplacian criteria with threshold 65 was expected to prune more nodes from the tree but in fact it did a slight change. Approaches using blurred images decrease false positive rate by making small and weak components disappear. For example, test CP2 labels only 1018 components for the same image No.218. We have to trade of the recall rate as some low contrast text will pass below the threshold and small one deformed too much.


The highest H-means obtained with a mix between CP3 and average laplacian using threshold at 70. This result only overcomes two other approaches from the competition at ICDAR 2013 \cite{ICDAR2013} because the low precision decreases the overall performance. In fact, the recall rate of our approaches are equivalent or higher than others. Pruning the tree using simple criteria maybe not enough as other methods which obtains high precision rate all used a verification step using a machine learning on the character candidate or text candidate .

\begin {table}[H]
\caption{Some of our best approach, compare with submission for challenge 2 of ICDAR 2013 Robust Reading Competition. (*): method uses a machine learning (NI): No information, there are methods submitted without detail explanation}\label{tab:Comparation} 
\begin{tabular}{|c|c|c|c|}
\hline 
\textbf{Method} & \textbf{Recall} & \textbf{Precision} & \textbf{Hmean (F1)} \\ 
\hline 
USTB TexStar(*) & 66.45\% & \textbf{88.47\%} & \textbf{75.89\%} \\ 
\hline 
Text Spotter(*) & 64.84\% & 87.51\% & 74.49\% \\ 
\hline 
CASIA NLPR(*)  & 68.24\% & 78.89\% & 73.18\% \\ 
\hline 
Text Detector CASIA(*) & 62.85\% & 84.70\% & 72.16\% \\ 
\hline 
I2R NUS FAR(*) & 69.00\% & 75.08\% & 71.91\% \\ 
\hline 
I2R NUS(*) & 66.17\% & 72.54\% & 69.21\% \\ 
\hline 
TH-TextLoc(*) & 65.19\% & 69.96\% & 67.49\% \\ 
\hline 
Text Detection(*) & 53.42\% & 74.15\% & 62.10\% \\ 
\hline 
CP3 + Average Laplacian, threshold = 70 & 63.96\% & 36.50\% & \textbf{46.48\%} \\ 
\hline 
Baseline(NI) & 34.74\% & 60.76\% & 44.21\%\\ 
\hline 
CP3 & 65.06\% & 27.75\% & 38.91\% \\ 
\hline 
Inkam(NI) & 35.27\% & 31.20\% & 33.11\% \\ 
\hline 
Propose approach \textbf{(CP1)} &\textbf{ \textbf{72.15\%}} & 6.11\% & 11.26\% \\
\hline 
\end{tabular} 
\end{table}

In term of execution time, most of the methods proposed did not gave reference about their speed except Neumann et al \cite{Neumann12} who arms to made a real time text detector \footnote{They define real-time text detection method is a method which has the processing time comparable with the time it would take a human to read the text.}. The processing time of \cite{Neumann12} for a particular image is 1380 ms (image 127 in the train set of ICDAR2013). Processing time of our original approach (CP1) for the same images is 5290.82 ms before optimization and 3249.41 ms after. This method needs more improvement.







