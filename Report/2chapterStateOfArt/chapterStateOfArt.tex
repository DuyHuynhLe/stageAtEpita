
\graphicspath{ {2chapterStateOfArt/image/} }
\chapter{State-of-the-art} \label{State-of-Art}
\section{Background}

To fully understand the importance of text detection and recognition approaches, this section provides information about some problems, applications and challenges.
\subsection{Text in images}
Text in images can be divided into 2 classes: scene text and  born-digital text. Scene text refers to text which is captured in its environment (such as sign, advertising, clothing). They were divided into focused text (Figure \ref{fig:focused}) and incidental text (Figure \ref{fig:incidental}) in the ICDAR Robust reading competition. Born-digital text (Figure \ref{fig:bornDigital}) includes text in born-digital images or graphically added into an images which can be found on overlay captions, subtitles and notation in videos and images on webs and email \cite{Karatzas.2011.ICDAR}.


\begin{figure}

	\begin{subfigure}[b]{0.3\textwidth}
	 	\includegraphics[width=5cm]{img1.jpg} \caption{Focused scene text}\label{fig:focused} \end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=5cm]{img2.jpg} \caption{Incidental scene text}\label{fig:incidental} \end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=5cm]{img3.jpg} \caption{Born-digital text}\label{fig:bornDigital} \end{subfigure}
	\centering
	\caption[Example of different \textit{text in image} problems] {Example of different \textit{text in image} problems.}
	\label{fig:example}
\end{figure}

\subsection{Application}
\par
Text in images and video contains valuable information. They can provide semantic information and useful features for many content-based image and video analysis tasks. Examples are content-based image search and multimedia retrieval, visual input and access, and industrial automation.

\begin{itemize}
\item{\textbf{Content-based image search and multimedia retrieval:}}

Texts in web images and videos are usually relevant to its content. Graphically added captions and captured scene texts usually give information about the situation of event such as location, people. Text recognition and keyword extraction in these resources improves multimedia retrieval. 

\item{\textbf{Visual input and access:}}

With the spread of imaging devices and the improvement of digital processors, a mobile visual input application can be realized. A mobile device can be use to digitize documents or automatically input name cards, bank cards, whiteboards \cite{Liang.2005.IJDAR}. As the input becomes automatic, user experience and efficiency are improved. 

Since signs, banners, advertises and product labels carry relevant information, automatic text recognition and translation can help people overcome language barriers or support visually impaired people in dally life. 

\item{\textbf{Industrial automation:}}

Text recognition in images of packages, letters, containers, houses, signs and maps is very useful for to industrial automation. For example, a mail sorting system will benefit addresses recognition. Another example, the automatic identification of container numbers will improve logistics efficiency \cite{He.2005.TITS}.

\end{itemize}

\subsection{Challenges}

Although many approaches have been proposed, and many advances have been made on text detection in digital born images, text detection in scene images is an open and challenging problem which has been receiving much attentions. This is a difficult task due to high variations of scene texts such as illumination, contrast, blur, distortion and variation of text content \cite{Liang.2005.IJDAR}. They are summarized in Table \ref{Chalenges}  .

 
\begin{table}
\caption{Scene text detection chalenges}\label{Chalenges}

\centering
\begin{tabular}{|l|l|}

\hline 
\textbf{Category} &\textbf{Challenges} \\ 
\hline 
Environment &Uneven illumination \\ 
	\cline{2-2} 
 	&Scene complexity \\ 
\hline 
Acquisition &Optical aberration \\ 
 	\cline{2-2} 
	 &Resolution \\  	
 	\cline{2-2} 
	 &Noise \\ 	 
 	\cline{2-2} 
	 &Compression \\ 	 
\hline 
Text content &Variation of orientation and curved text \\ 
	\cline{2-2}  
	 &Variation of fonts \\ 
	\cline{2-2} 
 	&Multilingual\\ 
\hline 
\end{tabular} 

\end{table}

\textbf{Scene complexity:} Man made objects in real life could have similar shapes and presentations to texts. Background complexity degrades the ability to discriminate text from non-text.

\textbf{Uneven illumination:} Images captured in natural environment are not always in good illumination conditions. They might suffer color distortions, degradation of visual feature which reduces the precision and make the segmentation and recognition tasks difficult.

\textbf{Optical aberration: } Most photos are not taken with an ideal lenses so optical aberrations might appear. Perspective distortion occurs when the sensor plane and the text plane are not parallel. Other distortions appear such as barrel, pincushion and mustache distortion due to quality of lenses. They can deform text sharp. Defocus occurs because focus can not be always maintain in normal working conditions and in some application such as visual aide for visually impaired. At flexible working conditions and focus-free cameras, defocusing and blurring of text happen. A moving object or a moving camera can also create motion blur, which are usually presence in video. Chromatic and coma aberration introduce false colors and reduce edge contrast. These optical aberrations make characters lost their sharp and their edges response. Since sharp edges response is required for character segmentation and recognition, blurring effect causes serious problem to the system \cite{Liang.2005.IJDAR}.

\textbf{Resolution:} Images that are taken by cameras usually have low resolution. This problem makes detection and segmentation difficult. While most OCR engines are tuned to resolution between 150 and 400 dpi, text in a video frame may be at or below 50 dpi \cite{Liang.2005.IJDAR}.

\textbf{Noise: } Noises degrade the quality of images. It is caused by shutter speed, sensor size and temperature. The lower shutter speed, the smaller sensor size and higher temperature, the greater the noise is. 

\textbf{Compression: } Most images captured by cameras are compressed because raw image will cost much storage space. The compression process is optimized for general uses instead of document analysis. An important issues is that it does not always preserve the topology and sharpness of contents.

\textbf{Variation of fonts: } Italic and script fonts may have overlap characters, which make it is difficult to recognize and segment them \cite{Liu.2011.ICDAR}. Variation of fonts introduce large within-class-variations, which may challenge approaches which use learning machine.

\textbf{Multilingual: } Languages that use the Latin alphabet have around tens characters but other languages such as Chinese, Japanese and Korean have thousands of characters. Other languages have characters shape connected or changed such as Arabic and Hindu. OCR in scanned multilingual documents remains a research problem \cite{Smith.2009.IWMO} and it is much more difficult in case of scene images.


\section{Existing approaches in scene text localization}
Text analysis and recognition include several sub-problems such as text localization, isolated character recognition and word recognition. They are processed individually (\cite{Campos.2010.CVPR} \cite{Chen.2004.CVPR} \cite{Liu.2006.CRA} \cite{Epshtein.2010.CVPR} \cite{xucheng.2013.pami}) or jointly \cite{Neumann12}. In these problems, the text detection modules is the most important part. It has been shown to be critically affect the performance of text in image retrieval algorithms \cite{Epshtein.2010.CVPR}. It is an open field and competitions have been held in order to evaluate state-of-the-art (Text Location Competition at ICDAR 2003, ICDAR 2005, ICDAR 2008, ICDAR 2013 and the recently ICDAR 2015). Text detection in scene images  poses significant challenges to state-of-the-art methods. The leading approach of text localization in text-focused scene images Task (task 2.1) in ICDAR 2013 obtains a recall of 66.45\% and precision of 88.47\% \cite{ICDAR.2013} which leave rooms for development. 


The main objective of text localization is to localize text components and group them into word candidates. These methods assume that text regions can be regarded as kind of uniform patterns. In consequence, there are some features that are invariant over this patterns such as color, edges, strokes width and texture. Related works can be divided into 3 catalogs: texture-based, connected-components and hybrids methods. 

\subsection{Methods}


Texture-based methods \cite{Chen.2004.CVPR} \cite{Lee.2011.ICDAR} \cite{wang.2011.iccv} \cite{Coates.2011.ICDAR} \cite{Wang.2012.ICPR} use a sliding window to look for all possible texts in the image. It is based on hypotheses that text regions in images have distinct textural properties from non-text regions (gradient distribution, texture and structure) that can be identified with a classifier such as Adaboost \cite{Lee.2011.ICDAR}, Random Ferns \cite{wang.2011.iccv}, linear SVM \cite{Coates.2011.ICDAR} or convolution neural network \cite{Wang.2012.ICPR}. These approaches may be expensive in computation when uses complex classification methods or large number of windows with different scales needs to be analyzed. These approaches are also limited by the training database and it is sensitive to text alignment orientation \cite{Pan.2011.TIP}. 


The connected-component-based (CCs) methods \cite{Epshtein.2010.CVPR} \cite{xucheng.2013.pami} \cite{Neumann.2011.ICDAR} \cite{Lee.2010.ICPR} group pixels having similar properties such as color, luminance or geometrical arrangement of edges, into connected regions. These methods often run an operation which segment the input images. The advantage of this approach is detection of texts regardless image properties such as scales, orientation and font type. It also provides a segmentation that can be useful in the OCR step. On the other hand, it might produce high amount of false positives. The obtained CCs are sometimes filtered using texture properties to remove CCs that cannot be characters before grouping into words to reduce false positives rate.


The hybrid methods combine both approaches. Pan et al. in \cite{Pan2009} uses HOG features and a Waldboost cascade classifier to generate a text confidence map, based on which connected-components extraction is done by a local binarization. Liu et al \cite{Liu.2006.CRA} is also a hybrid approach since it uses an edge detector on 3 channels to separate connected components which will be verified using Harr wavelet transform as texture characterization.

\subsection{Features}


Different features were used in text localization including color in different color spaces \cite{yi.2012.TIP} \cite{Roubtsova.2012.SPIE}, edges and gradient \cite{Pillai.2013.ICCPCT}, \cite{Chen.2004.CVPR}. New features such as stroke width \cite{Subramanian.2007.ICDAR} \cite{Epshtein.2010.CVPR} , corner \cite{Zhao.2011.TIP}, extremal region \cite{Neumann.2011.ICDAR} \cite{xucheng.2013.pami} or character appearance \cite{Ye.2014.CBDAR} \cite{Yi.2013.CVIU} have been recently studied.


\textbf{Color:} A readable text must have consistent and distinguishable color which contrasts with its background \cite{Liang.2005.IJDAR}. Color is widely used as feature to localize text \cite{Jain.1998.ICPR} \cite{Wang.2003.PR} \cite{Lee.2010.ICPR} although it seems to be sensitive to uneven lighting, multi-color and complicate textured texts. Nikolaou $et al.$ \cite{Nikolaou.2009.ISTI} simplifies the color images by color reduction and then clusters and groups connected components into candidates. Some authors use different color spaces to extract and analyze color feature, such as Hue-Saturation-Intensity (HSI) \cite{Garcia.2000.ICASSP}, HSI plus intensity gradient \cite{Neumann12} or Hue-Lightness-Saturation \cite{Karatzas.2004.ICPR}


\textbf{Edge and gradient approaches:} They assume that texts have strong edges against their background. Therefore edge detector and gradient map can be used to detect character candidate. Edge detectors and gradient \cite{Ye.2003.ICICS} \cite{Pillai.2013.ICCPCT} \cite{Shiva.2008.ICPR} was used widely in connected component. Sometimes they are used as a feature in sliding window-based approaches \cite{Chen.2004.CVPR} \cite{Hanif.2008.ICPR}.


\textbf{Stroke width:} From the assumption that text stroke should not vary much in a character, Epshtein et al proposed the stroke width transform (SWT) \cite{Epshtein.2010.CVPR}. The SWT returns a map which shows for each pixel the stroke width of the stroke to which it is most likely belonged. Recently, Mosleh et al \cite{Mosleh.2012.BMVC} showed that SWT could be improved by introducing a bandlet-based edge detector which enhances text edges and dismisses noisy and foliage edges.


\textbf{Corner: } With assumption that dense presences of corner points in text region, \cite{Zhao.2011.TIP} \cite{huang.2010.ICPR} used Harris corners detector as feature. Text candidate will then be detected using connected components-based approach \cite{huang.2010.ICPR} or analysis with a Decision tree classifier.


\textbf{Extremals region} has been widely explored in \cite{Neumann12} \cite{xucheng.2013.pami} \cite{Shi_2013:_MSER}. Using the same assumption that text components usually have significant color contrast with back-grounds and tend to form homogenous color regions, MSER algorithms adaptively detects stable color region to localize them as text candidate. The effectiveness of using MSERs as character candidates is the main advantage of this approach. The lead of ICDAR Robust reading contest 2013 is a MSERs-based approach.


%\textbf{Character appearance and texture features: }


%\par
%A typical text retrieval process includes 3 steps:
%\begin{itemize}\item
%Character retrieval: First, Text candidates must be highlighted and distinct from background. These candidate obtained by segmentation or by a characterization process, depends on the approach used (texture or CCs).
%\item
%Local classification: These candidates may be filtered out to remove non-text regions.
%\item
%Word grouping: Finally, character candidates will be grouped in to complete words. 
%\end{itemize}
%\par 
%We aim to make a reliable and fast text detectors, which can run on real time application such as detecting text in video. Many approaches \fxnote{cite} tried to detect all the texts candidates as possible then use a machine learning to remove the false positive before continue. This seams to be a redundant as the final purpose of text detection is to pass them into an OCR for recognition. This OCR will surely capable of verify if a candidate is truly a text.

